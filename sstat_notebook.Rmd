---
title: "Appendix A: R code to reproduce the study"
output: 
    pdf_document:
        number_sections: true
        keep_tex: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval=FALSE)
```

\newcommand{\beginappendix}{%
  \setcounter{table}{0}
  \renewcommand{\thetable}{A\arabic{table}}%
  \setcounter{figure}{0}
  \renewcommand{\thefigure}{A\arabic{figure}}%
  \setcounter{section}{0}
  \renewcommand{\thesection}{A\arabic{section}}%
}

\beginappendix


The easiest way to reproduce this study is to download, clone, or fork the GitHub repository at `github.com/ajrominger/paleo_supStat`. All scripts can then be run on new downloads of the PBDB and modifications to the analyses can be made. The repository is organized into directories `data` containing data and data-cleaning scripts; `R` containing R functions for general use; and `analysis` containing analysis scripts that use the data and R functions. The GitHub repository also contains a manuscript directory (`ms`), this document (`sstat_notebook.Rmd`), and an R script (`sstat_make.R`) that calls each data cleaning and analysis script in sequence to automatically reproduce the entire study. The `data`, `R`, and `analysis` directories can also be recreated from the scripts reproduced below.  

The accompanying explanations below (organized by the flow of data acquisition, cleaning, and then analysis) will help the user understand the purpose of each script/function such that they can reproduce the results, or modify the routine.

Lastly, this study depends on the contributed packages *divDyn*, *parallel*, and *R.utils* which should be installed from CRAN, and on a custom package *socorro* which must be installed from GitHub (using the *devtools* package):

```{r, eval=FALSE}
devtools::install_github('ajrominger/socorro')
```


# Getting the data

## PBDB API

To obtain the PBDB data we make use of the API in script `data/pbdb_data_get.R`, which accesses the API and cleans the data by:

- removing poorly lithified specimens
- removing collections at the basin scale
- including only fine-scale stratigraphy (below the "group" level)
- resolving taxonomy to the genus or subgenus level where available (storing genus or subgenus as `otu`)
- combining multiple records of the same OTU per collection
- importing standardized time bins from \url{fossilworks.org} (time bins are scraped with script `data/fossilworks_tbins_intervals.R`)

The data gathering script `data/pbdb_data_get.R` is shown below:

```{r code=readLines('data/pbdb_data_get.R')}
```


## Scraping fossilworks

The script to pull Alory's time bins (`data/fossilworks_tbins_intervals.R`) is below: 

```{r code=readLines('data/fossilworks_tbins_intervals.R')}
```


# Three timer and publication bias correction

Once the data have been downloaded and cleaned, we correct for incomplete and biased sampling with the script `data/pbdb_3TPub_make.R` which sources the function `R/make3TPub.R` to generate the main output: a matrix with time bins as rows, taxa (families in this case) as columns and bias-corrected richness as cells.

```{r code=readLines('data/pbdb_3TPub_make.R')}
```

Here is the guts of the `make3TPub` function
```{r code=readLines('R/make3TPub.R')}
```


Our publication correction is simple: we take the exponential of the residual of this relationship:
$$
\text{log(3T-corrected richness)} = \beta_0 + \beta_1 \text{log(number of publications)} + \epsilon
$$
The exponentiated residual amounts to dividing the three-timer corrected richness by a publication correction factor: $\text{(3T-corrected richness)} / e^{\hat{y}}$ where $\hat{y}$ is the predicted trend line from the above relationship.

So we can use this multiplicative publication correction factor in addition to the similarly multiplicative three-timer correction to bias-correct individual genus-level occurrences. This is important when we permute these bias-corrected genera across families to create a null set of d-statistics for our superstatistical fit.


We can check our correction against other popular methods.  In the script `analysis/pbdb_divCurve.R` we specifically compare simple rarefaction, with the SQS method, with our three-time and publication bias correction methods.  All these various methods have close agreement.  The script `analysis/pbdb_divCurve.R` is shown below:

```{r code=readLines('analysis/pbdb_divCurve.R')}
```


# Super statistical analysis of 3TPub-corrected PBDB data

Once data have been bias-corrected we can complete their super-statistical analysis.  We do that in the script `analysis/pbdb_sstat.R` shown here:

```{r code=readLines('analysis/pbdb_sstat.R')}
```


Now we can calculate a measure of goodness of fit with the Kolmogorov-Smirnov test statistics $D$. That is done in the script `analysis/pbdb_dperm.R`. This script uses a permutation approach to create a null distribution of test statistics.  The goal is to see if the good fit of super-statistics at the family and order levels is purely from the number of different groupings at those levels, regardless of the biology that might be going on to make those levels actually mechanistically meaningful.  So to achieve such a null, we permute orders across families, calculate the D-statistics of those permuted groupings, and compare to the real D-statistics from the actual biological groupings.  The script is shown below:

```{r code=readLines('analysis/pbdb_dperm.R')}
```


This permutation null test is in part motivated by the correlation between the genus richness of a family and its $\beta_k$ value.  We demonstrate this correlation in the script `analysis/pbdb_betaRichness.R` which is reproduced below:

```{r code=readLines('analysis/pbdb_betaRichness.R')}
```


Now that we are reasonably convinced that these superstatistical findings are not just an artifact of taxonomy or clade size, we can explore why we see deviations from super statistics with increasing taxonomic level. We first explore how well the Gaussian $p_k(x | \beta_k)$ fit at each taxonomic level in the script `analysis/pkx_diffK.R` shown here:

```{r code=readLines('analysis/pkx_diffK.R')}
```

We can also explore how well the $f(\beta_k)$ fit at different taxonomic levels in the script `analysis/pbeta_diffK.R` reproduced below:

```{r code=readLines('analysis/pbeta_diffK.R')}
```


Part of our argument about the failure of superstatistics at higher taxonomic levels is that these higher taxa aggregate increasingly disparate subtaxa.  To investigate this idea we look at the number of ecospace hypercubes represented by the average taxon at each taxonomic level in the script `analysis/pbdb_ecoEvoSpace.R` shown here:

```{r code=readLines('analysis/pbdb_ecoEvoSpace.R')}
```


# Helper functions

All the above analyses make use of helpful functions in the `R` directory. We reproduce those functions below:

```{r code=readLines('R/calcFlux.R')}
```

```{r code=readLines('R/gammaLS.R')}
```

```{r code=readLines('R/make3TPub.R')}
```

```{r code=readLines('R/normLS.R')}
```

```{r code=readLines('R/Px_gam.R')}
```

```{r code=readLines('R/sstat_likelihood_methods.R')}
```

```{r code=readLines('R/sstat_plot_methods.R')}
```

```{r code=readLines('R/sstatComp.R')}
```
